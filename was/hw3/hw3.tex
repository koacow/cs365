\documentclass[11pt]{article}

%\usepackage{geometry}
\usepackage[paper=a4paper, 
            left=20.0mm, right=20.0mm, 
            top=25.0mm, bottom=25.0mm]{geometry}
\pagestyle{empty}
\usepackage{graphicx}
\usepackage{fancyhdr, lastpage, bbding, pmboxdraw}
\usepackage[usenames,dvipsnames]{color}
\definecolor{darkblue}{rgb}{0,0,.6}
\definecolor{darkred}{rgb}{.7,0,0}
\definecolor{darkgreen}{rgb}{0,.6,0}
\definecolor{red}{rgb}{.98,0,0}
\usepackage[colorlinks,pagebackref,pdfusetitle,urlcolor=darkblue,citecolor=darkblue,linkcolor=darkred,bookmarksnumbered,plainpages=false]{hyperref}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\pagestyle{fancyplain}
\fancyhf{}
\lhead{ \fancyplain{}{Course Name} }
%\chead{ \fancyplain{}{} }
\rhead{ \fancyplain{}{\today} }
%\rfoot{\fancyplain{}{page \thepage\ of \pageref{LastPage}}}
\fancyfoot[RO, LE] {Page \thepage\ of \textcolor{black}{\pageref{LastPage}} }
\thispagestyle{plain}

%%%%%%%%%%%% LISTING %%%
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\usepackage{verbatim} % used to display code
\usepackage{fancyvrb}
\usepackage{acronym}
\usepackage{amsthm, amsmath, amssymb}
\usepackage{tikz}
    \usetikzlibrary{calc, arrows, arrows.meta, positioning}

\VerbatimFootnotes % Required, otherwise verbatim does not work in footnotes!

\definecolor{OliveGreen}{cmyk}{0.64,0,0.95,0.40}
\definecolor{CadetBlue}{cmyk}{0.62,0.57,0.23,0}
\definecolor{lightlightgray}{gray}{0.93}

\lstset{
	%language=bash,                          % Code langugage
	basicstyle=\ttfamily,                   % Code font, Examples: \footnotesize, \ttfamily
	keywordstyle=\color{OliveGreen},        % Keywords font ('*' = uppercase)
	commentstyle=\color{gray},              % Comments font
	numbers=left,                           % Line nums position
	numberstyle=\tiny,                      % Line-numbers fonts
	stepnumber=1,                           % Step between two line-numbers
	numbersep=5pt,                          % How far are line-numbers from code
	backgroundcolor=\color{lightlightgray}, % Choose background color
	frame=none,                             % A frame around the code
	tabsize=2,                              % Default tab size
	captionpos=t,                           % Caption-position = bottom
	breaklines=true,                        % Automatic line breaking?
	breakatwhitespace=false,                % Automatic breaks only at whitespace?
	showspaces=false,                       % Dont make spaces visible
	showtabs=false,                         % Dont make tabls visible
	columns=flexible,                       % Column format
	morekeywords={__global__, __device__},  % CUDA specific keywords
}

\newcommand{\question}[1]{\section*{\normalsize #1}}
% \newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
% \newcommand{\extraspace}[]{
%     \begin{center}
%         \textbf{Use this page for extra space.}
%     \end{center}
% }


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\begin{document}
\begin{center}
    {\Large \textsc{HW 3}}
\end{center}
\begin{center}
    Due: 11/14/2025 @ 11:59pm EST
\end{center}

\section*{\textbf{Disclaimer}}
I encourage you to work together, I am a firm believer that we are at our best (and learn better) when we communicate with our peers. Perspective is incredibly important when it comes to solving problems, and sometimes it takes talking to other humans (or rubber ducks in the case of programmers) to gain a perspective we normally would not be able to achieve on our own. The only thing I ask is that you report who you work with: this is \textbf{not} to punish anyone, but instead will help me figure out what topics I need to spend extra time on/who to help. When you turn in your solution (please use some form of typesetting: do \textbf{NOT} turn in handwritten solutions), please note who you worked with.









\question{Question 1: Stationary Distributions of Special Markov Chains (25 points)}
Consider a vector $\vec{q}\in \mathbb{R}^{1\times |V|}$ defined over the vertices of a Markov chain. To show that $\vec{q}$ is a stationary distribution, we must show that $\vec{q}$ is a probability vector. We must also show that $\vec{q}$ obeys the stationary distribution property $\vec{q} = \vec{q} \textbf{M}$:
\begin{enumerate}
    \item Show that for a connected and aperiodic markov chain with a transition matrix $M$ that is symmetric, the distribution $\vec{\pi} = \Big(\frac{1}{|V|}, \frac{1}{|V|}, \dots, \frac{1}{|V|}\Big)$ is a stationary distribution.

    \item Show that if we convert a connected, undirected graph into a Markov chain, the distribution $\vec{\pi}$ where $\pi[v] = \frac{deg(v)}{2|E|}$ is a stationary distribution.

    \item Show that if we convert a weighted, connected, undirected graph into a Markov chain, the distribution $\vec{\pi}$ where $\pi[v] = \frac{\sum\limits_{u\in \mathcal{N}_1(v)} w(u\rightarrow v)}{2\sum\limits_{(p\rightarrow q)\in E} w(p\rightarrow q)}$ is a stationary distribution where $\mathcal{N}_1(v)$ is the first neighborhood of vertex $v$ and $w(u\rightarrow v)$ is the weight of edge $u\rightarrow v$.
\end{enumerate}\newpage













\question{Question 2: The Pagerank Transform (25 points)}
Consider Markov chain with row-stochastic transition matrix $\textbf{M}$, none of which is a sink node. Let $\textbf{M}'$ be the result of applying the Pagerank transform to $\textbf{M}$ (i.e. compute $\textbf{M}'=\alpha \textbf{M} + (1-\alpha)\frac{1}{n}\textbf{1}$)
\begin{enumerate}
    \item Show that since $\textbf{M}$ is row-stochastic, then $\textbf{M}'$ is also row-stochastic.

    \item The stationary distribution $\vec{q}$ of $\textbf{M}'$ can be computed iteratively via an algorithm called the \textit{power method}:
\begin{lstlisting}[mathescape=true]
    $q^{(0)} = \frac{1}{n}\vec{1}$

    $t = 1$
    repeat
        $q^{(t)} = q^{(t-1)}\textbf{M}'$
        $\delta = ||q^{(t)} - q^{(t-1)}||_2$
        $t = t + 1$
    until $\delta < \epsilon$
\end{lstlisting}

\noindent However the computation $\vec{y} = \vec{x}\textbf{M}'$ can be done much faster (by avoiding calculating $\textbf{M}'$ using the following algorithm:
\begin{lstlisting}[mathescape=true]
    $\vec{y} = \alpha\vec{x}\textbf{M}$
    $\beta = ||\vec{x}||_1 - ||\vec{y}||_1$
    $\vec{y} = \vec{y} + \beta\frac{1}{n}\vec{1}$
\end{lstlisting}
Prove that this procedure computes $\vec{y} = \vec{x}\textbf{M}'$. Also, discuss the computational gains of using this procedure instead of $\vec{y} = \vec{x}\textbf{M}'$ as was used in the vanilla power method.

\end{enumerate}\newpage











\question{Question 3: Random Graphs (25 points)}
Consider a graph $G\sim G_{n,p}$. Derive an expression for $Pr[E]$, i.e. the probability of observing the specific graph $G$ from the $G_{n,p}$ model.
\newpage











\question{Question 4: More Random Graphs (25 points)}
Consider fixed graph $Q=(V, E_Q)$, and consider the distance between two graphs $G_1$ and $G_2$ to be the same as as earlier homework problem:
$$d(G_1, G_2) = |E_1 \backslash E_2| + |E_2 \backslash E_1|$$
Design a polynomial time algorithm to compute the expected distance between a graph $G~G_{n,p}$ and fixed graph $Q$. That is, design a polynomial time algorithm to compute:
$$\displaystyle \mathop{\mathbb{E}}_{G\sim G_{n,p}}[d(G, Q)] = \sum\limits_{G\sim G_{n,p}} Pr[G]d(G,Q)$$
\newpage












\question{Extra Credit: Exponential Rate of Converge of the Power Method (40 points)}
What we will show in this problem is that the rate of converge of the stationary distribution computation (i.e. the power method) for a symmetric row-stochastic matrix is exponential (this is incredibly fast) in the number of iterations $t$. Let $\textbf{M}$ be a square $n\times n$ symmetric row-stochastic matrix and let $\vec{\pi}$ be its stationary distribution (which from problem 1 is the uniform distribution).

\begin{enumerate}
    \item Show that if $\vec{e}_1, \vec{e}_2, \dots, \vec{e}_n$ are eigenvectors of $\textbf{M}$ with corresponding eigenvalues $\lambda_1, \lambda_2, \dots, \lambda_n$, then $\vec{e}_1, \vec{e}_2, \dots, \vec{e}_n$ are also eigenvectors of $\textbf{M}^t$ with corresponding eigenvalues $\lambda_1^t, \lambda_2^t, \dots, \lambda_n^t$

    \item Recall that when calculating the stationary distribution (i.e. with the power method), we are interested in how well $\vec{x}\textbf{M}^t$ approximates the stationary distribution $\vec{\pi}$. In other words, we are interested in $||\vec{x}\textbf{M}^t - \vec{\pi}||_2$. Given that $\vec{x}$ is a linear combination of the eigenvectors (i.e. $\vec{x}=\sum\limits_{i=1}^n \alpha_i\vec{e}_i$ with $\alpha_i=\vec{x}\vec{e}_i^T$ (careful, this could be negative!), show that $||\vec{x}\textbf{M}^t - \vec{\pi}||_2 \le \sqrt{n}\lambda_2^t$. Don't forget that $||\vec{y}||_2 = (\vec{y}\vec{y}^T)^{1/2}$ for any row vector $\vec{y}$.

\textit{Hint}: To show $\alpha_i\le 1$, you will likely want to use the Cauchy-Schwartz inequality for inner product: $\langle\vec{x},\vec{y}\rangle = \vec{x}\vec{y}^T \le ||\vec{x}||_2||\vec{y}||_2$
\end{enumerate}

\end{document}

