\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage[norelsize, linesnumbered, ruled, lined, boxed, commentsnumbered]{algorithm2e}
\usepackage{fullpage}


\pdfpagewidth 8.5in
\pdfpageheight 11in 

\topmargin 0in
\oddsidemargin 0in
\evensidemargin 0in

\newtheorem*{claim}{Claim}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\begin{document}

\begin{center}
    \textbf{CS365 Written Assignment 3} \\
    Khoa Cao \\
    Due November 14, 2025
\end{center}

\section*{Question 1}

\begin{enumerate}
    \item Let $\textbf{M}$ be the transition matrix of a Markov chain with $|V|$ states and let $\vec{pi}$ be the distribution:
    \[
        \vec{\pi} = \left[ \frac{1}{|V|}, \frac{1}{|V|}, \ldots, \frac{1}{|V|} \right]
    \]
    First, we show that $\vec{\pi}$ is a probability vector:
    \begin{align*}
        ||\vec{\pi}||_1 &= \sum_{i=1}^{|V|} \frac{1}{|V|} & \text{Definition of $\vec{\pi}$} \\
        &= \frac{|V|}{|V|} & \text{Sum of $|V|$ terms of $\frac{1}{|V|}$} \\
        &= 1 & \text{(Simplification)} \\
        &\Rightarrow \vec{\pi} \text{ is a probability vector}
    \end{align*}
    Then, we show that $\vec{\pi}\textbf{M} = \vec{\pi}$. Let $\vec{m}_i$ be the $i$-th column of $\textbf{M}$:
    \begin{align*}
        \vec{\pi}\textbf{M} &= \begin{bmatrix}
        \vec{\pi} \cdot \vec{m}_1 & \vec{\pi} \cdot \vec{m}_2 & \cdots & \vec{\pi} \cdot \vec{m}_n
    \end{bmatrix} & \text{Definition of matrix multiplication} \\
        &= \begin{bmatrix}
        \sum_{i=1}^{|V|} \frac{1}{|V|} m_{i1} & \sum_{i=1}^{|V|} \frac{1}{|V|} m_{i2} & \cdots & \sum_{i=1}^{|V|} \frac{1}{|V|} m_{in}
        \end{bmatrix} \\
        &= \begin{bmatrix}
            \frac{1}{|V|} \sum_{i=1}^{|V|} m_{i1} & \frac{1}{|V|} \sum_{i=1}^{|V|} m_{i2} & \cdots & \frac{1}{|V|} \sum_{i=1}^{|V|} m_{in}
        \end{bmatrix} & \text{Factoring out $\frac{1}{|V|}$} \\
        &= \begin{bmatrix}
            \frac{1}{|V|} \cdot 1 & \frac{1}{|V|} \cdot 1 & \cdots & \frac{1}{|V|} \cdot 1
        \end{bmatrix} & \textbf{M} \text{ is symmetric and row-stochastic} \\
        &= \vec{\pi} & \text{Definition of $\vec{\pi}$} \\
    \end{align*}
    Since we have shown that $\vec{\pi}$ is a probability vector and that $\vec{\pi}\textbf{M} = \vec{\pi}$, we conclude that $\vec{\pi}$ is a stationary distribution of the Markov chain with transition matrix $\textbf{M}$.

    \item Suppose we converted a connected, undirected graph into a Markov chain with. Let $\textbf{M}$ be the transition matrix of this Markov chain and let $\vec{\pi}$ be the distribution where
    \[
        \pi[v] = \frac{\deg(v)}{2|E|}
    \]
    First, we will show that $\vec{\pi}$ is a probability vector:
    \begin{align*}
        ||\vec{\pi}||_1 &= \sum_{v \in V} \frac{\deg(v)}{2|E|} & \text{Definition of $\vec{\pi}$} \\
        &= \frac{1}{2|E|} \sum_{v \in V} \deg(v) & \text{Factoring out $\frac{1}{2|E|}$} \\
        &= \frac{1}{2|E|} \cdot 2|E| & \text{every vertex is counted twice in the sum of degrees} \\
        &= 1 & \text{(Simplification)} \\
        &\Rightarrow \vec{\pi} \text{ is a probability vector}
    \end{align*}
    Next, we will show that $\vec{\pi} \textbf{M} = \vec{\pi}$. Let $\vec{m}_i$ be the $i$-th column of $\textbf{M}$:
    \begin{align*}
        \vec{\pi}\textbf{M} &= \begin{bmatrix}
            \vec{\pi} \cdot \vec{m}_1 & \vec{\pi} \cdot \vec{m}_2 & \cdots & \vec{\pi} \cdot \vec{m}_n
        \end{bmatrix} & \text{Definition of matrix multiplication} \\
        &= \begin{bmatrix}
            \sum_{i = 1}^{|V|} \pi[v_i] \cdot m_{i1} & \cdots & \sum_{i = 1}^{|V|} \pi[v_i] \cdot m_{in}
        \end{bmatrix} & \text{Expanding the dot products} \\
        &= \begin{bmatrix}
            \sum_{i = 1}^{|V|} \frac{\deg(v_i)}{2|E|} \cdot m_{i1} & \cdots & \sum_{i = 1}^{|V|} \frac{\deg(v_i)}{2|E|} \cdot m_{in}
        \end{bmatrix} & \text{Definition of $\pi[v_i]$} \\
        &= \begin{bmatrix}
            \frac{1}{2|E|} \sum_{i = 1}^{|V|} \deg(v_i) \cdot m_{i1} & \cdots & \frac{1}{2|E|} \sum_{i = 1}^{|V|} \deg(v_i) \cdot m_{in}
        \end{bmatrix} & \text{Factoring out $\frac{1}{2|E|}$} \\
        &= \begin{bmatrix}
            \frac{1}{2|E|} \sum_{i=1}^{|V|} \frac{1}{\deg(v_i)} \frac{w(v_i \rightarrow v_1)}{|\mathcal{N}_1(v_i)|} & \cdots & \frac{1}{2|E|} \sum_{i=1}^{|V|} \frac{1}{\deg(v_i)} \frac{w(v_i \rightarrow v_n)}{|\mathcal{N}_1(v_i)|}
        \end{bmatrix} & \text{Definition of } m_{ij} \\
        &= \begin{bmatrix}
            \frac{1}{2|E|} \sum_{v_i \in \mathcal{N}(v_1)} \frac{\deg(v_i)}{\deg(v_i)} & \cdots & \frac{1}{2|E|} \sum_{v_i \in \mathcal{N}(v_n)} \frac{\deg(v_i)}{\deg(v_i)}
        \end{bmatrix} & \text{Unweighted graph} \\
        &= \begin{bmatrix}
            \frac{1}{2|E|} \sum_{v_i \in \mathcal{N}(v_1)} 1 & \cdots & \frac{1}{2|E|} \sum_{v_i \in \mathcal{N}(v_n)} 1
        \end{bmatrix} & \text{Algebra} \\
        &= \begin{bmatrix}
            \frac{1}{2|E|} \cdot |\mathcal{N}(v_1)| & \frac{1}{2|E|} \cdot |\mathcal{N}(v_2)| & \cdots & \frac{1}{2|E|} \cdot |\mathcal{N}(v_n)|
        \end{bmatrix} & \text{Definition of summation} \\
        &= \begin{bmatrix}
            \frac{1}{2|E|} \cdot \deg(v_1) & \frac{1}{2|E|} \cdot \deg(v_2) & \cdots & \frac{1}{2|E|} \cdot \deg(v_n)
        \end{bmatrix} & \text{Count of neighbors equals degree} \\
        &= \vec{\pi} & \text{Definition of $\vec{\pi}$}
    \end{align*}
    Since we have shown that $\vec{\pi}$ is a probability vector and that $\vec{\pi}\textbf{M} = \vec{\pi}$, we conclude that $\vec{\pi}$ is a stationary distribution of the Markov chain derived from the connected, undirected graph.
    \item Suppose that we converted a connected, weighted, undirected graph into a Markov chain and let $\vec{\pi}$ be the distribution where:
    \[
        \pi[v] = \frac{\sum_{u \in \mathcal{N}_1(v)} w(u \rightarrow v)}{2 \sum_{(p \rightarrow q) \in E} w(p \rightarrow q)}
    \]
    First, we will show that $\vec{\pi}$ is a probability vector:
    \begin{align*}
        ||\vec{\pi}||_1 &= \sum_{v \in V} \pi[v] & \text{Definition of L1 norm} \\
        &= \sum_{v \in V} \frac{\sum_{u \in \mathcal{N}_1(v)} w(u \rightarrow v)}{2 \sum_{(p \rightarrow q) \in E} w(p \rightarrow q)} & \text{Definition of } \pi[v] \\
        &= \frac{\sum_{v \in V}\sum_{u \in \mathcal{N}_1(v)} w(u \rightarrow v)}{2 \sum_{(p \rightarrow q) \in E} w(p \rightarrow q)} & \text{Sum of fractions with same denominator}\\
        &= \frac{2 \sum_{(p \rightarrow q) \in E} w(p \rightarrow q)}{2 \sum_{(p \rightarrow q) \in E} w(p \rightarrow q)} & \text{Every edge is counted twice} \\ 
        &= 1 \\
        &\Rightarrow \vec{\pi} \text{ is a probability vector}
    \end{align*}
    Next, we will show that $\vec{\pi} \textbf{M} = \vec{\pi}$. Let $\vec{p} = \vec{\pi} \textbf{M}$ and $\vec{m}_i$ be the $i$-th column of $\textbf{M}$:
    \begin{align*}
        p[v_i] &= \vec{\pi} \cdot \vec{m_i} & \text{Definition of matrix multiplication} \\
        &= \sum_{j=1}^{|V|} \pi[v_j] \cdot m_{ji} & \text{Expanding the dot product} \\
        &= \sum_{j=1}^{|V|} \frac{\sum_{u \in \mathcal{N}_1(v_j)} w(u \rightarrow v_j)}{2 \sum_{(p \rightarrow q) \in E} w(p \rightarrow q)} \cdot m_{ji} & \text{Definition of } \pi[v] \\
        &= \sum_{j=1}^{|V|} \frac{\sum_{u \in \mathcal{N}_1(v_j)} w(u \rightarrow v_j)}{2 \sum_{(p \rightarrow q) \in E} w(p \rightarrow q)} \cdot \frac{w(v_j \rightarrow v_i)}{\sum_{u \in \mathcal{N}_1(v_j)} w(v_j \rightarrow u)} & \text{Definition of } m_{ji} \\
        &= \sum_{j=1}^{|V|} \frac{w(v_j \rightarrow v_i)}{2 \sum_{(p \rightarrow q) \in E} w(p \rightarrow q)} & \text{Dividing} \\
        &= \frac{\sum_{j=1}^{|V|} w(v_j \rightarrow v_i)}{2 \sum_{(p \rightarrow q) \in E} w(p \rightarrow q)} & \text{Combining fractions} \\
        &= \frac{\sum_{u \in \mathcal{N}_1(v_i)} w(u \rightarrow v_i)}{2 \sum_{(p \rightarrow q) \in E} w(p \rightarrow q)} & \text{Only neighbors of } v_i \text{ contribute} \\
        &= \pi[v_i] & \text{Definition of } \pi[v_i] \\
        &\Rightarrow \vec{p} = \vec{\pi} \textbf{M} = \vec{\pi}
    \end{align*}
    Since we have shown that $\vec{\pi}$ is a probability vector and that $\vec{\pi}\textbf{M} = \vec{\pi}$, we conclude that $\vec{\pi}$ is a stationary distribution of the Markov chain derived from the connected, weighted, undirected graph.
\end{enumerate}

\newpage

\section*{Question 2}
\begin{enumerate}
    \item Let $\textbf{M}$ be row-stochastic. We will show that $\textbf{M'} = \alpha \textbf{M} + \frac{1 - \alpha}{n} \textbf{1}$ is also row-stochastic. First we will show that the $ij$-th entry of $\textbf{M'}$ is between 0 and 1:
    \begin{align*}
        m_{ij}' &= \alpha m_{ij} + \frac{1 - \alpha}{n} \cdot 1 & \text{Matrix multiplication definition} \\
        &= \alpha m_{ij} + \frac{1 - \alpha}{n} & \text{Simplification} \\
        \Rightarrow min(m_{ij}') &= \frac{1 - \alpha}{n} \geq 0 & \text{Since } 1 - \alpha \geq 0 \text{ and } n > 0 \text{ } (1) \\
        max(m_{ij}') &= \alpha + \frac{1 - \alpha}{n} \\
        &= \frac{\alpha n + 1 - \alpha}{n} & \text{Finding common denominator} \\
        &\leq \frac{n}{n} & \text{Since } \alpha \leq 1 \\
        &\leq 1 & \text{Simplification } (2) \\
        (1), (2) &\Rightarrow 0 \leq m_{ij}' \leq 1
    \end{align*}
    Next, we will show that the sum of each row in $\textbf{M'}$ is equal to 1:
    \begin{align*}
        \sum_{j=1}^{n} m_{ij}' &= \sum_{j=1}^{n} \left( \alpha m_{ij} + \frac{1 - \alpha}{n} \right) & \text{Using results from above} \\
        &= \alpha \sum_{j=1}^{n} m_{ij} + \sum_{j=1}^{n} \frac{1 - \alpha}{n} & \text{Distributing the summation} \\
        &= \alpha \cdot 1 + (1 - \alpha) & \text{Since } \textbf{M} \text{ is row-stochastic} \\
        &= 1 & \text{Simplification}
    \end{align*}
    Since we have shown that each entry of $\textbf{M'}$ is between 0 and 1 and that the sum of each row in $\textbf{M'}$ is equal to 1, we conclude that $\textbf{M'}$ is row-stochastic.
    \item We will show that the algorithm computes $\vec{y} = \vec{x} \textbf{M'}$ where $\textbf{M'} = \alpha \textbf{M} + \frac{1 - \alpha}{n} \textbf{1}$.
    \begin{align*}
        \vec{y} &= \vec{y} + \beta \frac{1}{n} \vec{1} & \text{Line 3 of the algorithm} \\
        &= \alpha \vec{x} \textbf{M} + \left(||\vec{x}||_1 - ||\alpha \vec{x} \textbf{M}||_1 \right) \frac{1}{n} \vec{1} & \text{Substituting } \vec{y} \text{ and } \beta \\
        &= \alpha \vec{x} \textbf{M} + \left(1 - \alpha ||\vec{x} \textbf{M}||_1 \right) \frac{1}{n} \vec{1} & \vec{x} \text{ is a probability vector} \\
        &= \alpha \vec{x} \textbf{M} + \left(1 - \alpha \cdot 1 \right) \frac{1}{n} \vec{1} & \textbf{M} \text{ is row-stochastic and } \vec{x} \text{ is a probability vector} \\
        &= \alpha \vec{x} \textbf{M} + \frac{1 - \alpha}{n} \vec{1} & \text{Simplification} \\
        &= \vec{x} \left( \alpha \textbf{M} + \frac{1 - \alpha}{n} \textbf{1} \right) & \text{Factoring out } \vec{x} \\
        &= \vec{x} \textbf{M'} & \text{Definition of } \textbf{M'}
    \end{align*}
    This algorithm saves time by avoiding the need to compute $\textbf{M'}$ explicitly, which would require $O(n^2)$ time.
\end{enumerate}

\newpage

\section*{Question 3}

Let $G \sim G_{n,p}$ be a graph generated from the $G_{n, p}$ model. Let $v_1, v_2, \ldots, v_n$ be the vertices of $G$ and $E_{i,j}$ be the outcome of the edge between $v_i$ and $v_j$ (where $i < j$).
Then, the probability of observing a specific graph $G$ with a given edge set is determined by:
\begin{align*}
    Pr[E_{i,j}] &= \begin{cases}
        p & \text{if } (v_i, v_j) \in E \\
        1 - p & \text{if } (v_i, v_j) \notin E
    \end{cases} & \text{Definition of } G_{n,p} \\
    \Rightarrow Pr[E] &= \prod_{1 \leq i < j \leq n} Pr[E_{i,j}] & \text{Independence of edges} \\
    &= \prod_{(v_i, v_j) \in E} p \cdot \prod_{(v_i, v_j) \notin E} (1 - p) & \text{Separating edges in } E \text{ and not in } E \\
    &= p^{|E|} (1 - p)^{\binom{n}{2} - |E|} & \text{Counting the number of edges} \\
\end{align*}
Therefore, the probability of observing a specific graph $G$ with edge set $E$ in the $G_{n,p}$ model is given by $Pr[E] = p^{|E|} (1 - p)^{\binom{n}{2} - |E|}$.

\newpage

\section*{Question 4}
Consider the expected distance between a fixed graph $Q$ and a random graph $G \sim G_{n,p}$:
\begin{align*}
    \mathbb{E}_{G \sim G_{n,p}}[d(G, Q)] &= \mathbb{E}_{G \sim G_{n,p}} \left[ |E_G \setminus E_Q| + |E_Q \setminus E_G| \right] & \text{Definition of distance} \\
    &= \mathbb{E}_{G \sim G_{n,p}} \left[ |E_G \setminus E_Q| \right] + \mathbb{E}_{G \sim G_{n,p}} \left[ |E_Q \setminus E_G| \right] & \text{Linearity of expectation} \\
\end{align*}
We compute each term separately. For the first term, note that $|E_G \setminus E_Q|$ counts the edges present in $G$ but not in $Q$. 
The possible number of such edges ranges from 0 to $\binom{n}{2} - |E_Q|$. Each edge not in $Q$ has a probability $p$ of being present in $G$. Thus, the expected number of such edges is:
\begin{align*}
    \mathbb{E}_{G \sim G_{n, p}} \left[ |E_G \setminus E_Q| \right] &= \sum_{i=0}^{\binom{n}{2} - |E_Q|} i \cdot \binom{\binom{n}{2} - |E_Q|}{i} p^i (1 - p)^{\binom{n}{2} - |E_Q| - i} & \text{Definition of expectation} \\
    &= p \cdot \left[\binom{n}{2} - |E_Q|\right] & \text{Expected value of a Binomial distribution}
\end{align*}

For the second term, note that $|E_Q \setminus E_G|$ counts the edges present in $Q$ but not in $G$.
The possible number of such edges ranges from 0 to $|E_Q|$. Each edge in $Q$ has a probability $1 - p$ of being absent in $G$. Thus, the expected number of such edges is:
\begin{align*}
    \mathbb{E}_{G \sim G_{n, p}} \left[ |E_Q \setminus E_G| \right] &= \sum_{i=0}^{|E_Q|} i \cdot \binom{|E_Q|}{i} (1 - p)^i p^{|E_Q| - i} & \text{Definition of expectation} \\
    &= (1 - p) \cdot |E_Q| & \text{Expected value of a Binomial distribution}
\end{align*}
Combining both terms, we have:
\begin{align*}
    \mathbb{E}_{G \sim G_{n,p}}[d(G, Q)] &= p \cdot \left[\binom{n}{2} - |E_Q|\right] + (1 - p) \cdot |E_Q| & \text{Combining both terms} \\
\end{align*}

Therefore, to compute the expected distance between a fixed graph $Q$ and a random graph $G \sim G_{n,p}$, we can count the number of edges in $Q$ and use the formula above.
This algorithm runs in $O(n^2)$ time since counting the edges in $Q$ takes $O(n^2)$ time.
\newpage

\section*{Extra Credit}
\begin{enumerate}
    \item Let $\textbf{M} \in \mathcal{R}^{n \times n}$ be a row-stochastic transition matrix. Consider an arbitrary eigenvector $\vec{e}$ of 
    $\textbf{M}$ with corresponding eigenvalue $\lambda$. We will show that $\vec{e}$ is also an eigenvector of $\textbf{M}^t$ with eigenvalue $\lambda^t$ for any integer $t \geq 1$ using induction on $t$.
    \begin{itemize}
        \item \textbf{Base Case}: Let $t = 1$. Then, we have:
        \begin{align*}
            \vec{e} \textbf{M}^1 &= \vec{e} \textbf{M} & \text{Matrix exponentation} \\
            &= \lambda \vec{e} & \text{Definition of eigenvector} \\
            &\Rightarrow \vec{e} \text{ is an eigenvector of } \textbf{M}^1 \text{ with eigenvalue } \lambda^1
        \end{align*}
        Thus, the base case holds.
        \item \textbf{Inductive Step}: Assume that for some integer $k \geq 1$, $\vec{e}$ is an eigenvector of $\textbf{M}^k$ with eigenvalue $\lambda^k$. 
        We will show that $\vec{e}$ is also an eigenvector of $\textbf{M}^{k+1}$ with eigenvalue $\lambda^{k+1}$.
        \begin{align*}
            \vec{e} \textbf{M}^{k+1} &= \vec{e} \textbf{M}^k \textbf{M} & \text{Matrix exponentation} \\
            &= \lambda^k \vec{e} \textbf{M} & \text{Inductive Hypothesis} \\
            &= \lambda^k \lambda \vec{e} & \text{Definition of eigenvector} \\
            &= \lambda^{k+1} \vec{e} & \text{Simplification} \\
            &\Rightarrow \vec{e} \text{is an eigenvector of } \textbf{M}^{k+1} \text{ with eigenvalue } \lambda^{k+1}
        \end{align*}
        Thus, the inductive step holds.
    \end{itemize}
    \item Let $\vec{\pi}$ be a stationary distribution of $\textbf{M}$. We will show that $||\vec{x} \textbf{M}^t - \vec{\pi}||_2 \leq \sqrt{n} \lambda_2^t$ for any probability vector $\vec{x}$, where $\lambda_2$ is the second largest eigenvalue of $\textbf{M}$.
    Consider the difference $\vec{x} \textbf{M}^t - \vec{\pi}$:
    \begin{align*}
        \vec{x} \textbf{M}^t - \vec{\pi} &= \left(\sum_{i=1}^{n} \alpha_i \vec{e}_i\right) \textbf{M}^t - \vec{\pi} & \text{Expanding } \vec{x} \text{ in terms of eigenvectors} \\
        &= \sum_{i=1}^{n} \alpha_i \lambda_i^t \vec{e}_i - \vec{\pi} & \text{Using result from part (a)} \\
        &= \sum_{i=2}^{n} \alpha_i \lambda_i^t \vec{e}_i & \vec{\pi} = \alpha_1 \vec{e}_1 \text{ and } \lambda_1 = 1\\
        \Rightarrow \norm{\vec{x} \textbf{M}^t - \vec{\pi}}_2^2 &= \norm{\sum_{i=2}^{n} \alpha_i \lambda_i^t \vec{e}_i}_2^2 \\
        &= \sum_{i=2}^{n} \norm{\alpha_i \lambda_i^t \vec{e}_i}_2^2 & \text{Pythagorean theorem} \\
        &= \sum_{i=2}^{n} \alpha_i^2 \lambda_i^{2t} & \vec{e}_i \text{ are unit vectors} \\
        &\leq \lambda_2^{2t} \sum_{i=2}^{n} \alpha_i^2 & \text{Since } |\lambda_i| \leq \lambda_2 \text{ for } i \geq 2 \\
    \end{align*}
    Now, we will bound the value of $\alpha_i$. Note that:
    \begin{align*}
        \alpha_i &= \vec{x} \vec{e}_i^T & \text{Definition of } \alpha_i \\
        &\leq ||\vec{x}||_2 \cdot ||\vec{e}_i||_2 & \text{Cauchy-Schwarz inequality} \\
        &\leq ||\vec{x}||_2 & \vec{e}_i \text{ are unit vectors} \\
        &\leq \sum_{j=1}^{n} (x_j)^2 & \text{Definition of L2 norm} \\
        &\leq \sum_{j=1}^{n} (x_j) & \text{Since } 0 \leq x_j \leq 1 \text{ for probability vectors} \\
        &\leq 1
    \end{align*}
    Then:
    \begin{align*}
        \norm{\vec{x} \textbf{M}^t - \vec{\pi}}_2^2 &\leq \lambda_2^{2t} \sum_{i=2}^{n} ||\vec{x}||_2^2 & \text{Substituting the bound on } \alpha_i \\
        &\leq \lambda_2^{2t} (n-1) ||\vec{x}||_2^2 & \text{There are } n-1 \text{ terms in the sum} \\
        &\leq \lambda_2^{2t} n ||\vec{x}||_2^2 & \text{Simplification} \\
        &\leq \lambda_2^{2t} n & \text{Since } ||\vec{x}||_2^2 \leq 1 \text{ for probability vectors} \\
        \Rightarrow \norm{\vec{x} \textbf{M}^t - \vec{\pi}}_2 &\leq \sqrt{n} \lambda_2^t & \text{Taking the square root of both sides}
    \end{align*}
    Therefore, we have shown that $||\vec{x} \textbf{M}^t - \vec{\pi}||_2 \leq \sqrt{n} \lambda_2^t$ for any probability vector $\vec{x}$.
\end{enumerate}
\end{document}