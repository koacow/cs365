\documentclass[11pt, fleqn]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage[norelsize, linesnumbered, ruled, lined, boxed, commentsnumbered]{algorithm2e}
\usepackage{fullpage}


\pdfpagewidth 8.5in
\pdfpageheight 11in 

\topmargin 0in
\oddsidemargin 0in
\evensidemargin 0in

\newtheorem*{claim}{Claim}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}

\begin{document}

\begin{center}
    \textbf{CS365 Written Assignment 1} \\
    Khoa Cao \\
    Due 
\end{center}

\section*{Question 1}
\begin{enumerate}
    \item Let $\textbf{X} = \textbf{AB}$ where $\textbf{A} \in \mathbb{R}^{m \times n}$ and $\textbf{B} \in \mathbb{R}^{n \times p}$. Let $\vec{a_i}$ be the $i^{th}$ row of $\textbf{A}$ and $\vec{b_j}$ be the $j^{th}$ column of $\textbf{B}$. Then, the $ij^{th}$ entry of $\textbf{X}$ is given by
    \begin{align*}
        X_{ij} &= \vec{a_i} \cdot \vec{b_j} & \text{definition of matrix multiplication} \\
        X_{ij} &= \sum_{k=1}^{n} A_{ik} \cdot B_{kj} & \text{definition of dot product}
    \end{align*}
    Differentiating w.r.t. $A_{lp}$ gives:
    \begin{align*}
        \pd{X_{ij}}{A_{lp}} &= \pd{(\sum_{k=1}^{n} A_{ik} \cdot B_{kj})}{A_{lp}} & \text{from previous result} \\
        &= \begin{cases}
            B_{pj} & \text{if } i = l \text{ and } k = p \\
            0 & \text{otherwise}
        \end{cases} \\
        \Rightarrow \pd{\textbf{X}}{A_{lp}} &= \begin{bmatrix}
            0 & \cdots & 0 & \cdots & 0 \\
            \vdots & \ddots & \vdots & & \vdots \\
            B_{p1} & \cdots & B_{pj} & \cdots & B_{pn} \\
            \vdots & & \vdots & \ddots & \vdots \\
            0 & \cdots & 0 & \cdots & 0
        \end{bmatrix} & \text{writing derivative in matrix form} \\
        \Rightarrow \pd{\textbf{X}}{\textbf{A}} &= \begin{bmatrix}
            \pd{\textbf{X}}{A_{11}} & \pd{\textbf{X}}{A_{12}} & \cdots & \pd{\textbf{X}}{A_{1n}} \\
            \pd{\textbf{X}}{A_{21}} & \pd{\textbf{X}}{A_{22}} & \cdots & \pd{\textbf{X}}{A_{2n}} \\
            \vdots & \vdots & \ddots & \vdots \\
            \pd{\textbf{X}}{A_{m1}} & \pd{\textbf{X}}{A_{m2}} & \cdots & \pd{\textbf{X}}{A_{mn}}
        \end{bmatrix} & \text{writing derivative in matrix form} \\
        &= \begin{bmatrix}
            \begin{bmatrix}
                0 & \cdots & 0 & \cdots & 0 \\
                \vdots & \ddots & \vdots & & \vdots \\
                B_{11} & \cdots & B_{1j} & \cdots & B_{1n} \\
                \vdots & & \vdots & \ddots & \vdots \\
                0 & \cdots & 0 & \cdots & 0
            \end{bmatrix} & \cdots \\
            \vdots & \ddots \\
        \end{bmatrix} & \text{substituting previous result} \\
    \end{align*}
    \item Let $\textbf{X} = \textbf{AB}$ where $\textbf{A} \in \mathbb{R}^{m \times n}$ and $\textbf{B} \in \mathbb{R}^{n \times p}$. Let $\vec{a_i}$ be the $i^{th}$ row of $\textbf{A}$ and $\vec{b_j}$ be the $j^{th}$ column of $\textbf{B}$. Then, the $ij^{th}$ entry of $\textbf{X}$ is given by
    \begin{align*}
        X_{ij} &= \vec{a_i} \cdot \vec{b_j} & \text{definition of matrix multiplication} \\
        X_{ij} &= \sum_{k=1}^{n} A_{ik} \cdot B_{kj} & \text{definition of dot product}
    \end{align*}
    Differentiating w.r.t. $B_{lp}$ gives:
    \begin{align*}
        \pd{X_{ij}}{B_{lp}} &= \pd{(\sum_{k=1}^{n} A_{ik} \cdot B_{kj})}{B_{lp}} & \text{from previous result} \\
        &= \begin{cases}
            A_{il} & \text{if } k = l \text{ and } j = p \\
            0 & \text{otherwise}
        \end{cases} \\
        \Rightarrow \pd{\textbf{X}}{B_{lp}} &= \begin{bmatrix}
            0 & \cdots & A_{1l} & \cdots & 0 \\
            \vdots & \ddots & \vdots & & \vdots \\
            0 & \cdots & A_{il} & \cdots & 0 \\
            \vdots & \ddots & \vdots & & \vdots \\
            0 & \cdots & A_{ml} & \cdots & 0 \\
        \end{bmatrix} & \text{writing derivative in matrix form} \\
        \Rightarrow \pd{\textbf{X}}{\textbf{A}} &= \begin{bmatrix}
            \pd{\textbf{X}}{B_{11}} & \pd{\textbf{X}}{B_{12}} & \cdots & \pd{\textbf{X}}{B_{1n}} \\
            \pd{\textbf{X}}{B_{21}} & \pd{\textbf{X}}{B_{22}} & \cdots & \pd{\textbf{X}}{B_{2n}} \\
            \vdots & \vdots & \ddots & \vdots \\
            \pd{\textbf{X}}{B_{m1}} & \pd{\textbf{X}}{B_{m2}} & \cdots & \pd{\textbf{X}}{B_{mn}}
        \end{bmatrix} & \text{writing derivative in matrix form} \\
        &= \begin{bmatrix}
            \begin{bmatrix}
                0 & \cdots & A_{11} & \cdots & 0 \\
                \vdots & \ddots & \vdots & & \vdots \\
                0 & \cdots & A_{i1} & \cdots & 0 \\
                \vdots & \ddots & \vdots & & \vdots \\
                0 & \cdots & A_{m1} & \cdots & 0 \\
            \end{bmatrix} & \cdots \\
            \vdots & \ddots \\
        \end{bmatrix} & \text{substituting previous result} \\
    \end{align*}
    \item First, we find the scalar equation for the $L_p$ norm of $\vec{x}$.
    \begin{align*}
        ||\vec{x}||_p^p &= \left[ \left(\sum_{i=1}^{n} |x_i|^p \right)^{\frac{1}{p}} \right]^p & \text{definition of } L_p \text{ norm} \\
        &= \sum_{i=1}^{n} |x_i|^p & \text{simplifying} \\
    \end{align*}
    Then, differentiating w.r.t. $x_i$ gives:
    \begin{align*}
        \pd{||\vec{x}||_p^p}{x_i} &= p |x_i|^{p - 1} \cdot \text{sign}(x_i) & \text{chain rule} \\
        \Rightarrow \pd{||\vec{x}||_p}{\vec{x}} &= \begin{bmatrix}
            \pd{||\vec{x}||_p^p}{x_1} & \pd{||\vec{x}||_p^p}{x_2} & \cdots & \pd{||\vec{x}||_p^p}{x_n}
        \end{bmatrix} & \text{writing derivative in vector form} \\
        &= \begin{bmatrix}
            p |x_1|^{p - 1} \cdot sgn(x_1) & \cdots & p |x_n|^{p - 1} \cdot sgn(x_n)
        \end{bmatrix} & \text{substituting previous result} \\
    \end{align*}
    \item Let $\textbf{A} \in \mathbb{R}^{n \times n}$. First, we find the scalar equation for $\vec{x}^T \textbf{A} \vec{x}$.
    \begin{align*}
        \vec{x}^T \textbf{A} \vec{x} &= \begin{bmatrix}
            \vec{x} \cdot \vec{a_1} & \vec{x} \cdot \vec{a_2} & \cdots & \vec{x} \cdot \vec{a_n}
        \end{bmatrix} \cdot \vec{x} & \text{definition of matrix multiplication} \\
        &= \sum_{i=1}^{n} (\vec{x} \cdot \vec{a_i}) x_i & \text{definition of dot product} \\
        &= \sum_{i=1}^{n} \left( \sum_{j=1}^{n} A_{ji} x_j \right) x_i & \text{expanding } \vec{x} \cdot \vec{a_i} \\
        &= \sum_{i=1}^{n} \sum_{j=1}^{n} A_{ji} x_j x_i & \text{simplifying} \\
    \end{align*}
    Then, differentiating w.r.t. $x_k$ gives:
    \begin{align*}
        \pd{\vec{x}^T \textbf{A} \vec{x}}{x_k} &= \sum_{i=1}^{n} \sum_{j=1}^{n} A_{ji} \pd{(x_j x_i)}{x_k} & \text{linearity of differentiation} \\
        &= \sum_{i=1}^{n} A_{ki} x_i + \sum_{j=1}^{n} A_{jk} x_j & \text{only terms with } i = k \text{ or } j = k \text{ remain} \\
        \Rightarrow \pd{\vec{x}^T \textbf{A} \vec{x}}{\vec{x}} &= \begin{bmatrix}
            \pd{\vec{x}^T \textbf{A} \vec{x}}{x_1} & \pd{\vec{x}^T \textbf{A} \vec{x}}{x_2} & \cdots & \pd{\vec{x}^T \textbf{A} \vec{x}}{x_n}
        \end{bmatrix} & \text{writing derivative in vector form} \\
        &= \begin{bmatrix}
            \sum_{i} A_{1i} x_i + \sum_{j} A_{j1} x_j & \cdots & \sum_{i} A_{ni} x_i + \sum_{j} A_{jn} x_j
        \end{bmatrix} & \text{substituting previous result} \\
        &= \begin{bmatrix}
            \sum_{i} A_{1i} x_i & \cdots & \sum_{i} A_{ni} x_i
        \end{bmatrix} + \begin{bmatrix}
            \sum_{j} A_{j1} x_j & \cdots & \sum_{j} A_{jn} x_j
        \end{bmatrix} & \text{separating the sums} \\
        &= \textbf{A} \vec{x} + \textbf{A}^T \vec{x} & \text{rewriting in matrix form} \\
        &= (\textbf{A} + \textbf{A}^T) \vec{x} & \text{factoring out } \vec{x} \\
    \end{align*}
    \item Using the result from part (d), we have:
    \begin{align*}
        \vec{x}^T \textbf{A} \vec{x} &= \sum_{i} \sum_{j} A_{ji} x_j x_i & \text{from part (d)} \\
    \end{align*}
    Differentiating w.r.t. $A_{kl}$ gives:
    \begin{align*}
        \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{kl}} &= \sum_{i} \sum_{j} \pd{(A_{ji} x_j x_i)}{A_{kl}} & \text{linearity of differentiation} \\
        &= x_l x_k & \text{only the term with } j = k \text{ and } i = l \text{ remains} \\
        \Rightarrow \pd{\vec{x}^T \textbf{A} \vec{x}}{\textbf{A}} &= \begin{bmatrix}
            \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{11}} & \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{12}} & \cdots & \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{1n}} \\
            \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{21}} & \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{22}} & \cdots & \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{2n}} \\
            \vdots & \vdots & \ddots & \vdots \\
            \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{n1}} & \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{n2}} & \cdots & \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{nn}}
        \end{bmatrix} & \text{writing derivative in matrix form} \\
        &= \begin{bmatrix}
            x_1 x_1 & x_1 x_2 & \cdots & x_1 x_n \\
            x_2 x_1 & x_2 x_2 & \cdots & x_2 x_n \\
            \vdots & \vdots & \ddots & \vdots \\
            x_n x_1 & x_n x_2 & \cdots & x_n x_n
        \end{bmatrix} & \text{substituting previous result} \\
        &= \vec{x} \vec{x}^T & \text{rewriting in vector form} \\
    \end{align*}
\end{enumerate}

\newpage

\section*{Question 2}
\begin{enumerate}
    \item Given: 
    \begin{align*}
        Q(X | \theta, \pi) &= \sum_{\vec{x_i} \in X}\sum_{j=1}^{k} \gamma_{ij} \left( \log(\pi_{j}) + \log(Pr[\vec{x_i} | \vec{\mu}_j, \Sigma_j])\right)
        s.t. \sum_{j = 1}^{k} \pi_{ij} &= 1
    \end{align*}
    We will use Lagrange multipliers to find the value of $\mu_m$ that maximizes $Q(X | \theta, \pi)$. Consider:
    \begin{align*}
        Q(X | \theta, \pi) &= \sum_{\vec{x_i} \in X}\sum_{j=1}^{k} \gamma_{ij} \left( \log(\pi_{j}) + \log(Pr[\vec{x_i} | \vec{\mu}_j, \Sigma_j])\right) - \lambda \left(\sum_{j = 1}^{k} \pi_{ij} - 1\right) & \text{adding constraint} \\
        \Rightarrow \pd{Q}{\mu_m} &= \sum_{\vec{x_i} \in X}\sum_{j=1}^{k} \gamma_{ij} \left( \log(\pi_{j}) + \log(Pr[\vec{x_i} | \vec{\mu}_j, \Sigma_j])\right) & \text{constant terms disappear} \\
        &= \pd{}{\vec{\mu_m}}\sum_{\vec{x_i} \in X}\sum_{j=1}^{k} \gamma_{ij} \log \pi_j + \gamma_{ij} \log(Pr[\vec{x_i} | \vec{\mu}_j, \Sigma_j]) & \text{foiling} \\
        &= \sum_{\vec{x_i} \in X}\sum_{j=1}^{k} \gamma_{ij} \pd{}{\vec{\mu_m}} \log(Pr[\vec{x_i} | \vec{\mu_j}, \Sigma_j]) & \text{constant terms disappear} \\
        &= \sum_{\vec{x_i} \in X} \gamma_{im} \pd{}{\vec{\mu_m}} \log(Pr[\vec{x_i} | \vec{\mu_m}, \Sigma_m]) & \text{only terms with } j = m \text{ remain} \\
        &= \sum_{\vec{x_i} \in X} \gamma_{im} \pd{}{\vec{\mu_m}} \log \frac{e^{-\frac{1}{2} (\vec{x_i} - \vec{\mu_m})^T \boldsymbol{\Sigma}^{-1} (\vec{x_i} - \vec{\mu_m})}}{\sqrt{(2\pi)^k \left|\Sigma\right|}} & \text{substituting pdf} \\
        &= \sum_{\vec{x_i} \in X} \gamma_{im} \pd{}{\vec{\mu_m}} \log e^{-\frac{1}{2} (\vec{x_i} - \vec{\mu_m})^T \boldsymbol{\Sigma}^{-1} (\vec{x_i} - \vec{\mu_m})} - \log \left(\sqrt{(2\pi)^k \left|\Sigma\right|}\right) & \text{using log rules} \\
        &= \sum_{\vec{x_i} \in X} \gamma_{im} \pd{}{\vec{\mu_m}} -\frac{1}{2} (\vec{x_i} - \vec{\mu_m})^T \boldsymbol{\Sigma}^{-1} (\vec{x_i} - \vec{\mu_m}) & \text{constant terms disappear} \\
        &= \sum_{\vec{x_i} \in X} -\frac{\gamma_{im}}{2} \left( \left[ \boldsymbol{\Sigma}^{-1} + {\boldsymbol{\Sigma}^{-1}}^T \right] (\vec{x_i} - \vec{\mu_m}) \right) \pd{}{\vec{\mu_m}} (\vec{x_i} - \vec{\mu_m}) & \text{result from 1.4} \\
        &= \sum_{\vec{x_i} \in X} -\frac{\gamma_{im}}{2} \left( -2\boldsymbol{\Sigma}^{-1} (\vec{x_i} - \vec{\mu_m}) \right) & \text{since } \boldsymbol{\Sigma} \text{ is symmetric} \\
        &= \sum_{\vec{x_i} \in X} \gamma_{im} \boldsymbol{\Sigma}^{-1} (\vec{x_i} - \vec{\mu_m}) & \text{simplifying} \\
    \end{align*}
    Setting the equation above to $0$ and solving for $\vec{\mu_m}$ gives:
    \begin{align*}
        \Rightarrow \pd{Q}{\vec{\mu_m}} &= 0 & \text{setting derivative to } 0 \text{ for maximization} \\
        \Rightarrow \sum_{\vec{x_i} \in X} \gamma_{im} \boldsymbol{\Sigma}^{-1} (\vec{x_i} - \vec{\mu_m}) &= 0 & \text{from previous result} \\
        \Rightarrow \boldsymbol{\Sigma}^{-1} \sum_{\vec{x_i} \in X} \gamma_{im} (\vec{x_i} - \vec{\mu_m}) &= 0 & \text{factoring out } \boldsymbol{\Sigma}^{-1} \\
        \Rightarrow \sum_{\vec{x_i} \in X} \gamma_{im} (\vec{x_i} - \vec{\mu_m}) &= 0 & \text{columns of } \boldsymbol{\Sigma}^{-1} \text{ are linearly independent} \\
        \Rightarrow \sum_{\vec{x_i} \in X} \gamma_{im} \vec{x_i} - \sum_{\vec{x_i} \in X} \gamma_{im} \vec{\mu_m} &= 0 & \text{distributing the sum} \\
        \Rightarrow \sum_{\vec{x_i} \in X} \gamma_{im} \vec{x_i} - \vec{\mu_m} \sum_{\vec{x_i} \in X} \gamma_{im} &= 0 & \text{factoring out } \vec{\mu_m} \\
        \Rightarrow \vec{\mu_m} &= \frac{\sum_{\vec{x_i} \in X} \gamma_{im} \vec{x_i}}{\sum_{\vec{x_i} \in X} \gamma_{im}} & \text{solving for } \vec{\mu_m} \\
    \end{align*}

    \item Similarly, we will solve for the MLE estimate, $\boldsymbol{\Sigma}_m^*$. Consider:
    \begin{align*}
        \pd{Q}{\boldsymbol{\Sigma}_m} &= \sum_{\vec{x_i} \in X} \gamma_{im} \pd{}{\boldsymbol{\Sigma}_m} \left[ \log e^{-\frac{1}{2} (\vec{x_i} - \vec{\mu_m})^T \boldsymbol{\Sigma}^{-1} (\vec{x_i} - \vec{\mu_m})} - \log \left(\sqrt{(2\pi)^k \left|\boldsymbol{\Sigma}\right|}\right) \right] & \text{from part (a)} \\
        &= \sum_{\vec{x_i} \in X} \gamma_{im} \pd{}{\boldsymbol{\Sigma}_m} \left[ -\frac{1}{2} (\vec{x_i} - \vec{\mu_m})^T \boldsymbol{\Sigma}^{-1} (\vec{x_i} - \vec{\mu_m}) - \frac{k}{2}\log (2\pi) - \frac{1}{2} \log \left|\boldsymbol{\Sigma}\right| \right] & \text{log rules} \\
        &= \sum_{\vec{x_i} \in X} -\frac{\gamma_{im}}{2} \pd{}{\boldsymbol{\Sigma}_m} \left[ (\vec{x_i} - \vec{\mu_m})^T \boldsymbol{\Sigma}^{-1} (\vec{x_i} - \vec{\mu_m}) + k\log (2\pi) + \log \left|\boldsymbol{\Sigma}\right| \right] & \text{linearity of differentiation} \\
        &= \sum_{\vec{x_i} \in X} -\frac{\gamma_{im}}{2} \left[ \pd{}{\boldsymbol{\Sigma}_m} (\vec{x_i} - \vec{\mu_m})^T \boldsymbol{\Sigma}^{-1} (\vec{x_i} - \vec{\mu_m}) + \pd{}{\boldsymbol{\Sigma}_m} \log \left|\boldsymbol{\Sigma}\right| \right] & \text{linearity of differentiation} \\
        &= \sum_{\vec{x_i} \in X} -\frac{\gamma_{im}}{2} \left[ -\boldsymbol{\Sigma}^{-T} (\vec{x_i} - \vec{\mu_m})(\vec{x_i} - \vec{\mu_m})^T \boldsymbol{\Sigma}^{-T}  + \boldsymbol{\Sigma}^{-T} \right] & \text{matrix cookbook} \\
        &= \sum_{\vec{x_i} \in X} \frac{\gamma_{im}}{2} \left[ \boldsymbol{\Sigma}^{-1} (\vec{x_i} - \vec{\mu_m})(\vec{x_i} - \vec{\mu_m})^T \boldsymbol{\Sigma}^{-1}  - \boldsymbol{\Sigma}^{-1} \right] & \text{since } \boldsymbol{\Sigma} \text{ is symmetric} \\
    \end{align*}
    Setting the equation above to $0$ and solving for $\boldsymbol{\Sigma}_m$ gives:
    \begin{align*}
        \pd{Q}{\boldsymbol{\Sigma}_m} &= 0 & \text{setting derivative to } 0 \text{ for maximization} \\
        \Rightarrow \sum_{\vec{x_i} \in X} \frac{\gamma_{im}}{2} \left[ \boldsymbol{\Sigma}^{-1} (\vec{x_i} - \vec{\mu_m})(\vec{x_i} - \vec{\mu_m})^T \boldsymbol{\Sigma}^{-1}  - \boldsymbol{\Sigma}^{-1} \right] &= 0 & \text{from previous result} \\
        \Rightarrow \sum_{\vec{x_i} \in X} \gamma_{im} \boldsymbol{\Sigma}^{-1} (\vec{x_i} - \vec{\mu_m})(\vec{x_i} - \vec{\mu_m})^T \boldsymbol{\Sigma}^{-1} &= \sum_{\vec{x_i} \in X} \gamma_{im} \boldsymbol{\Sigma}^{-1} & \text{rearranging} \\
        \Rightarrow \boldsymbol{\Sigma}^{-1} \left( \sum_{\vec{x_i} \in X} \gamma_{im} (\vec{x_i} - \vec{\mu_m})(\vec{x_i} - \vec{\mu_m})^T \right) \boldsymbol{\Sigma}^{-1} &= \left( \sum_{\vec{x_i} \in X} \gamma_{im} \right) \boldsymbol{\Sigma}^{-1}  & \text{factoring out } \boldsymbol{\Sigma}^{-1} \\
        \Rightarrow \sum_{\vec{x_i} \in X} \gamma_{im} (\vec{x_i} - \vec{\mu_m})(\vec{x_i} - \vec{\mu_m})^T &= \boldsymbol{\Sigma} \left( \sum_{\vec{x_i} \in X} \gamma_{im} \right) & \text{multiplying both sides by } \boldsymbol{\Sigma} \\
        \Rightarrow \frac{\sum_{\vec{x_i} \in X} \gamma_{im} (\vec{x_i} - \vec{\mu_m})(\vec{x_i} - \vec{\mu_m})^T}{\sum_{\vec{x_i} \in X} \gamma_{im}} &= \boldsymbol{\Sigma}_m^* & \text{solving for } \boldsymbol{\Sigma}_m \\
    \end{align*}
\end{enumerate}

\newpage

\section*{Question 3}
\begin{enumerate}
    \item Let $\textbf{M} \in \mathbb{R}^{n \times n}$ be a symmetric matrix. Suppose $\textbf{M}$ has eigenvectors $\vec{v_1}, \vec{v_2}$ with distinct eigenvalues $\lambda_1, \lambda_2$. We will show that
    these eigenvectors are orthogonal, i.e. $\vec{v_1} \cdot \vec{v_2} = 0$. Consider:
    \begin{align*}
        \textbf{M} \vec{v_1} \cdot \vec{v_2} &= \vec{v_1} \cdot \textbf{M}^T \vec{v_2} & \text{given lemma} \\
        \Rightarrow \textbf{M} \vec{v_1} \cdot \vec{v_2} &= \vec{v_1} \cdot \textbf{M} \vec{v_2} & \text{since } \textbf{M} \text{ is symmetric} \\
        \Rightarrow \lambda_1 \vec{v_1} \cdot \vec{v_2} &= \lambda_2 \vec{v_1} \cdot \vec{v_2} & \text{definition of eigenvector} \\
        \Rightarrow \lambda_1 \vec{v_1} \cdot \vec{v_2} - \lambda_2 \vec{v_1} \cdot \vec{v_2} &= 0 & \text{rearranging} \\
        \Rightarrow (\lambda_1 - \lambda_2) (\vec{v_1} \cdot \vec{v_2}) &= 0 & \text{factoring} \\
        \Rightarrow \vec{v_1} \cdot \vec{v_2} &= 0 & \text{since } \lambda_1 \neq \lambda_2 \\
    \end{align*}
    Therefore, eigenvectors of a square symmetric matrix with distinct eigenvalues are orthogonal.
    \item Let $\textbf{A} \in \mathbb{R}^{m \times n}$ be any matrix. We will show that $\textbf{A} \textbf{A}^T \in \mathbb{R}^{m \times m}$ and $\textbf{A}^T \textbf{A} \in \mathbb{R}^{n \times n}$ are symmetric. Consider:
    \begin{align*}
        (\textbf{A} \textbf{A}^T)^T &= (\textbf{A}^T)^T \textbf{A}^T & \text{transpose of a matrix product} \\
        &= \textbf{A} \textbf{A}^T & \text{double transpose} \\
        &\Rightarrow \textbf{A} \textbf{A}^T \text{ is symmetric by definition} \\
    \end{align*}
    Similarly, consider:
    \begin{align*}
        (\textbf{A}^T \textbf{A})^T &= \textbf{A}^T (\textbf{A})^T & \text{transpose of a matrix product} \\
        &= \textbf{A}^T \textbf{A} & \text{double transpose} \\
        &\Rightarrow \textbf{A}^T \textbf{A} \text{ is symmetric by definition} \\
    \end{align*}

    \item Let $\textbf{A} = \textbf{U} \boldsymbol{\Sigma} \textbf{V}^T$ be the SVD of $\textbf{A}$, $\vec{v}$ be an eigenvector of $\textbf{A} \textbf{A}^T$ with eigenvalue $\lambda$.
    Consider $\textbf{A} \textbf{A}^T$:
    \begin{align*}
        \textbf{A} \textbf{A}^T &= \textbf{U} \boldsymbol{\Sigma} \textbf{V}^T (\textbf{U} \boldsymbol{\Sigma} \textbf{V}^T)^T & \text{substituting SVD of } \textbf{A} \\
        &= \textbf{U} \boldsymbol{\Sigma} \textbf{V}^T \textbf{V} \boldsymbol{\Sigma}^T \textbf{U}^T & \text{transpose of a matrix product} \\
        &= \textbf{U} \boldsymbol{\Sigma} \boldsymbol{\Sigma}^T \textbf{U}^T & \text{since } \textbf{V}^T \textbf{V} = \textbf{I} \\
        &= \textbf{U} \boldsymbol{\Sigma}^2 \textbf{U}^T & \text{since } \boldsymbol{\Sigma} \text{ is diagonal} \\
        \Rightarrow \textbf{AA}^T \textbf{U} &= \textbf{U} \boldsymbol{\Sigma}^2 \textbf{U}^T \textbf{U} & \text{multiplying both sides by } \textbf{U} \\
        \Rightarrow \textbf{AA}^T \textbf{U} &= \textbf{U} \boldsymbol{\Sigma}^2 & \text{since } \textbf{U}^T \textbf{U} = \textbf{I} \\\
        \Rightarrow \begin{bmatrix}
            \textbf{AA}^T \vec{u_1} & \textbf{AA}^T \vec{u_2} & \cdots & \textbf{AA}^T \vec{u_m}
        \end{bmatrix} &= \begin{bmatrix}
            \sigma_1^2 \vec{u_1} & \sigma_2^2 \vec{u_2} & \cdots & \sigma_m^2 \vec{u_m}
        \end{bmatrix} & \text{writing in terms of columns} \\
    \end{align*}
    Therefore, the eigenvectors of $\textbf{A} \textbf{A}^T$ are the columns of $\textbf{U}$ and the corresponding eigenvalues are the squares of the singular values in $\boldsymbol{\Sigma}$.
    \item Similarly, consider $\textbf{A}^T \textbf{A}$:
    \begin{align*}
        \textbf{A}^T \textbf{A} &= (\textbf{U} \boldsymbol{\Sigma} \textbf{V}^T)^T \textbf{U} \boldsymbol{\Sigma} \textbf{V}^T & \text{substituting SVD of } \textbf{A} \\
        &= \textbf{V} \boldsymbol{\Sigma}^T \textbf{U}^T \textbf{U} \boldsymbol{\Sigma} \textbf{V}^T & \text{transpose of a matrix product} \\
        &= \textbf{V} \boldsymbol{\Sigma}^T \boldsymbol{\Sigma} \textbf{V}^T & \text{since } \textbf{U}^T \textbf{U} = \textbf{I} \\
        &= \textbf{V} \boldsymbol{\Sigma}^2 \textbf{V}^T & \text{since } \boldsymbol{\Sigma} \text{ is diagonal} \\
        \Rightarrow \textbf{A}^T\textbf{A} \textbf{V} &= \textbf{V} \boldsymbol{\Sigma}^2 \textbf{V}^T \textbf{V} & \text{multiplying both sides by } \textbf{V} \\
        \Rightarrow \textbf{A}^T\textbf{A} \textbf{V} &= \textbf{V} \boldsymbol{\Sigma}^2 & \text{since } \textbf{V}^T \textbf{V} = \textbf{I} \\
        \Rightarrow \begin{bmatrix}
            \textbf{A}^T\textbf{A} \vec{v_1} & \textbf{A}^T\textbf{A} \vec{v_2} & \cdots & \textbf{A}^T\textbf{A} \vec{v_n}
        \end{bmatrix} &= \begin{bmatrix}
            \sigma_1^2 \vec{v_1} & \sigma_2^2 \vec{v_2} & \cdots & \sigma_n^2 \vec{v_n}
        \end{bmatrix} & \text{writing in terms of columns} \\
    \end{align*}
    Therefore, the eigenvectors of $\textbf{A}^T \textbf{A}$ are the columns of $\textbf{V}$ and the corresponding eigenvalues are the squares of the singular values in $\boldsymbol{\Sigma}$.
\end{enumerate}

\newpage

\section*{Extra Credit}
\end{document}