\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage[norelsize, linesnumbered, ruled, lined, boxed, commentsnumbered]{algorithm2e}
\usepackage{fullpage}

\pdfpagewidth 8.5in
\pdfpageheight 11in 

\topmargin 0in
\oddsidemargin 0in
\evensidemargin 0in

\newtheorem*{claim}{Claim}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}

\begin{document}

\begin{center}
    \textbf{CS365 Written Assignment 1} \\
    Khoa Cao \\
    Due 
\end{center}

\section*{Question 1}
\begin{enumerate}
    \item Let $\textbf{X} = \textbf{AB}$ where $\textbf{A} \in \mathbb{R}^{m \times n}$ and $\textbf{B} \in \mathbb{R}^{n \times p}$. Let $\vec{a_i}$ be the $i^{th}$ row of $\textbf{A}$ and $\vec{b_j}$ be the $j^{th}$ column of $\textbf{B}$. Then, the $ij^{th}$ entry of $\textbf{X}$ is given by
    \begin{align*}
        X_{ij} &= \vec{a_i} \cdot \vec{b_j} & \text{definition of matrix multiplication} \\
        X_{ij} &= \sum_{k=1}^{n} A_{ik} \cdot B_{kj} & \text{definition of dot product}
    \end{align*}
    Differentiating w.r.t. $A_{lp}$ gives:
    \begin{align*}
        \pd{X_{ij}}{A_{lp}} &= \pd{(\sum_{k=1}^{n} A_{ik} \cdot B_{kj})}{A_{lp}} & \text{from previous result} \\
        &= \begin{cases}
            B_{pj} & \text{if } i = l \text{ and } k = p \\
            0 & \text{otherwise}
        \end{cases} \\
        \Rightarrow \pd{\textbf{X}}{A_{lp}} &= \begin{bmatrix}
            0 & \cdots & 0 & \cdots & 0 \\
            \vdots & \ddots & \vdots & & \vdots \\
            B_{p1} & \cdots & B_{pj} & \cdots & B_{pn} \\
            \vdots & & \vdots & \ddots & \vdots \\
            0 & \cdots & 0 & \cdots & 0
        \end{bmatrix} & \text{writing derivative in matrix form} \\
        \Rightarrow \pd{\textbf{X}}{\textbf{A}} &= \begin{bmatrix}
            \pd{\textbf{X}}{A_{11}} & \pd{\textbf{X}}{A_{12}} & \cdots & \pd{\textbf{X}}{A_{1n}} \\
            \pd{\textbf{X}}{A_{21}} & \pd{\textbf{X}}{A_{22}} & \cdots & \pd{\textbf{X}}{A_{2n}} \\
            \vdots & \vdots & \ddots & \vdots \\
            \pd{\textbf{X}}{A_{m1}} & \pd{\textbf{X}}{A_{m2}} & \cdots & \pd{\textbf{X}}{A_{mn}}
        \end{bmatrix} & \text{writing derivative in matrix form} \\
        &= \begin{bmatrix}
            \begin{bmatrix}
                0 & \cdots & 0 & \cdots & 0 \\
                \vdots & \ddots & \vdots & & \vdots \\
                B_{11} & \cdots & B_{1j} & \cdots & B_{1n} \\
                \vdots & & \vdots & \ddots & \vdots \\
                0 & \cdots & 0 & \cdots & 0
            \end{bmatrix} & \cdots \\
            \vdots & \ddots \\
        \end{bmatrix} & \text{substituting previous result} \\
    \end{align*}
    \item Let $\textbf{X} = \textbf{AB}$ where $\textbf{A} \in \mathbb{R}^{m \times n}$ and $\textbf{B} \in \mathbb{R}^{n \times p}$. Let $\vec{a_i}$ be the $i^{th}$ row of $\textbf{A}$ and $\vec{b_j}$ be the $j^{th}$ column of $\textbf{B}$. Then, the $ij^{th}$ entry of $\textbf{X}$ is given by
    \begin{align*}
        X_{ij} &= \vec{a_i} \cdot \vec{b_j} & \text{definition of matrix multiplication} \\
        X_{ij} &= \sum_{k=1}^{n} A_{ik} \cdot B_{kj} & \text{definition of dot product}
    \end{align*}
    Differentiating w.r.t. $B_{lp}$ gives:
    \begin{align*}
        \pd{X_{ij}}{B_{lp}} &= \pd{(\sum_{k=1}^{n} A_{ik} \cdot B_{kj})}{B_{lp}} & \text{from previous result} \\
        &= \begin{cases}
            A_{il} & \text{if } k = l \text{ and } j = p \\
            0 & \text{otherwise}
        \end{cases} \\
        \Rightarrow \pd{\textbf{X}}{B_{lp}} &= \begin{bmatrix}
            0 & \cdots & A_{1l} & \cdots & 0 \\
            \vdots & \ddots & \vdots & & \vdots \\
            0 & \cdots & A_{il} & \cdots & 0 \\
            \vdots & \ddots & \vdots & & \vdots \\
            0 & \cdots & A_{ml} & \cdots & 0 \\
        \end{bmatrix} & \text{writing derivative in matrix form} \\
        \Rightarrow \pd{\textbf{X}}{\textbf{A}} &= \begin{bmatrix}
            \pd{\textbf{X}}{B_{11}} & \pd{\textbf{X}}{B_{12}} & \cdots & \pd{\textbf{X}}{B_{1n}} \\
            \pd{\textbf{X}}{B_{21}} & \pd{\textbf{X}}{B_{22}} & \cdots & \pd{\textbf{X}}{B_{2n}} \\
            \vdots & \vdots & \ddots & \vdots \\
            \pd{\textbf{X}}{B_{m1}} & \pd{\textbf{X}}{B_{m2}} & \cdots & \pd{\textbf{X}}{B_{mn}}
        \end{bmatrix} & \text{writing derivative in matrix form} \\
        &= \begin{bmatrix}
            \begin{bmatrix}
                0 & \cdots & A_{11} & \cdots & 0 \\
                \vdots & \ddots & \vdots & & \vdots \\
                0 & \cdots & A_{i1} & \cdots & 0 \\
                \vdots & \ddots & \vdots & & \vdots \\
                0 & \cdots & A_{m1} & \cdots & 0 \\
            \end{bmatrix} & \cdots \\
            \vdots & \ddots \\
        \end{bmatrix} & \text{substituting previous result} \\
    \end{align*}
    \item First, we find the scalar equation for the $L_p$ norm of $\vec{x}$.
    \begin{align*}
        ||\vec{x}||_p^p &= \left[ \left(\sum_{i=1}^{n} |x_i|^p \right)^{\frac{1}{p}} \right]^p & \text{definition of } L_p \text{ norm} \\
        &= \sum_{i=1}^{n} |x_i|^p & \text{simplifying} \\
    \end{align*}
    Then, differentiating w.r.t. $x_i$ gives:
    \begin{align*}
        \pd{||\vec{x}||_p^p}{x_i} &= p |x_i|^{p - 1} \cdot \text{sign}(x_i) & \text{chain rule} \\
        \Rightarrow \pd{||\vec{x}||_p}{\vec{x}} &= \begin{bmatrix}
            \pd{||\vec{x}||_p^p}{x_1} & \pd{||\vec{x}||_p^p}{x_2} & \cdots & \pd{||\vec{x}||_p^p}{x_n}
        \end{bmatrix} & \text{writing derivative in vector form} \\
        &= \begin{bmatrix}
            p |x_1|^{p - 1} \cdot sgn(x_1) & \cdots & p |x_n|^{p - 1} \cdot sgn(x_n)
        \end{bmatrix} & \text{substituting previous result} \\
    \end{align*}
    \item Let $\textbf{A} \in \mathbb{R}^{n \times n}$. First, we find the scalar equation for $\vec{x}^T \textbf{A} \vec{x}$.
    \begin{align*}
        \vec{x}^T \textbf{A} \vec{x} &= \begin{bmatrix}
            \vec{x} \cdot \vec{a_1} & \vec{x} \cdot \vec{a_2} & \cdots & \vec{x} \cdot \vec{a_n}
        \end{bmatrix} \cdot \vec{x} & \text{definition of matrix multiplication} \\
        &= \sum_{i=1}^{n} (\vec{x} \cdot \vec{a_i}) x_i & \text{definition of dot product} \\
        &= \sum_{i=1}^{n} \left( \sum_{j=1}^{n} A_{ji} x_j \right) x_i & \text{expanding } \vec{x} \cdot \vec{a_i} \\
        &= \sum_{i=1}^{n} \sum_{j=1}^{n} A_{ji} x_j x_i & \text{simplifying} \\
    \end{align*}
    Then, differentiating w.r.t. $x_k$ gives:
    \begin{align*}
        \pd{\vec{x}^T \textbf{A} \vec{x}}{x_k} &= \sum_{i=1}^{n} \sum_{j=1}^{n} A_{ji} \pd{(x_j x_i)}{x_k} & \text{linearity of differentiation} \\
        &= \sum_{i=1}^{n} A_{ki} x_i + \sum_{j=1}^{n} A_{jk} x_j & \text{only terms with } i = k \text{ or } j = k \text{ remain} \\
        \Rightarrow \pd{\vec{x}^T \textbf{A} \vec{x}}{\vec{x}} &= \begin{bmatrix}
            \pd{\vec{x}^T \textbf{A} \vec{x}}{x_1} & \pd{\vec{x}^T \textbf{A} \vec{x}}{x_2} & \cdots & \pd{\vec{x}^T \textbf{A} \vec{x}}{x_n}
        \end{bmatrix} & \text{writing derivative in vector form} \\
        &= \begin{bmatrix}
            \sum_{i} A_{1i} x_i + \sum_{j} A_{j1} x_j & \cdots & \sum_{i} A_{ni} x_i + \sum_{j} A_{jn} x_j
        \end{bmatrix} & \text{substituting previous result} \\
        &= \begin{bmatrix}
            \sum_{i} A_{1i} x_i & \cdots & \sum_{i} A_{ni} x_i
        \end{bmatrix} + \begin{bmatrix}
            \sum_{j} A_{j1} x_j & \cdots & \sum_{j} A_{jn} x_j
        \end{bmatrix} & \text{separating the sums} \\
        &= \textbf{A} \vec{x} + \textbf{A}^T \vec{x} & \text{rewriting in matrix form} \\
        &= (\textbf{A} + \textbf{A}^T) \vec{x} & \text{factoring out } \vec{x} \\
    \end{align*}
    \item Using the result from part (d), we have:
    \begin{align*}
        \vec{x}^T \textbf{A} \vec{x} &= \sum_{i} \sum_{j} A_{ji} x_j x_i & \text{from part (d)} \\
    \end{align*}
    Differentiating w.r.t. $A_{kl}$ gives:
    \begin{align*}
        \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{kl}} &= \sum_{i} \sum_{j} \pd{(A_{ji} x_j x_i)}{A_{kl}} & \text{linearity of differentiation} \\
        &= x_l x_k & \text{only the term with } j = k \text{ and } i = l \text{ remains} \\
        \Rightarrow \pd{\vec{x}^T \textbf{A} \vec{x}}{\textbf{A}} &= \begin{bmatrix}
            \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{11}} & \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{12}} & \cdots & \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{1n}} \\
            \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{21}} & \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{22}} & \cdots & \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{2n}} \\
            \vdots & \vdots & \ddots & \vdots \\
            \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{n1}} & \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{n2}} & \cdots & \pd{\vec{x}^T \textbf{A} \vec{x}}{A_{nn}}
        \end{bmatrix} & \text{writing derivative in matrix form} \\
        &= \begin{bmatrix}
            x_1 x_1 & x_1 x_2 & \cdots & x_1 x_n \\
            x_2 x_1 & x_2 x_2 & \cdots & x_2 x_n \\
            \vdots & \vdots & \ddots & \vdots \\
            x_n x_1 & x_n x_2 & \cdots & x_n x_n
        \end{bmatrix} & \text{substituting previous result} \\
        &= \vec{x} \vec{x}^T & \text{rewriting in vector form} \\
    \end{align*}
\end{enumerate}

\newpage

\section*{Question 3}
\begin{enumerate}
    \item Let $\textbf{M} \in \mathbb{R}^{n \times n}$ be a symmetric matrix. Suppose $\textbf{M}$ has eigenvectors $\vec{v_1}, \vec{v_2}$ with distinct eigenvalues $\lambda_1, \lambda_2$. We will show that
    these eigenvectors are orthogonal, i.e. $\vec{v_1} \cdot \vec{v_2} = 0$. Consider:
    \begin{align*}
        \textbf{M} \vec{v_1} \cdot \vec{v_2} &= \vec{v_1} \cdot \textbf{M}^T \vec{v_2} & \text{given lemma} \\
        \Rightarrow \textbf{M} \vec{v_1} \cdot \vec{v_2} &= \vec{v_1} \cdot \textbf{M} \vec{v_2} & \text{since } \textbf{M} \text{ is symmetric} \\
        \Rightarrow \lambda_1 \vec{v_1} \cdot \vec{v_2} &= \lambda_2 \vec{v_1} \cdot \vec{v_2} & \text{definition of eigenvector} \\
        \Rightarrow \lambda_1 \vec{v_1} \cdot \vec{v_2} - \lambda_2 \vec{v_1} \cdot \vec{v_2} &= 0 & \text{rearranging} \\
        \Rightarrow (\lambda_1 - \lambda_2) (\vec{v_1} \cdot \vec{v_2}) &= 0 & \text{factoring} \\
        \Rightarrow \vec{v_1} \cdot \vec{v_2} &= 0 & \text{since } \lambda_1 \neq \lambda_2 \\
    \end{align*}
    Therefore, eigenvectors of a square symmetric matrix with distinct eigenvalues are orthogonal.
    \item Let $\textbf{A} \in \mathbb{R}^{m \times n}$ be any matrix. We will show that $\textbf{A} \textbf{A}^T \in \mathbb{R}^{m \times m}$ and $\textbf{A}^T \textbf{A} \in \mathbb{R}^{n \times n}$ are symmetric. Consider:
    \begin{align*}
        (\textbf{A} \textbf{A}^T)^T &= (\textbf{A}^T)^T \textbf{A}^T & \text{transpose of a matrix product} \\
        &= \textbf{A} \textbf{A}^T & \text{double transpose} \\
        &\Rightarrow \textbf{A} \textbf{A}^T \text{ is symmetric by definition} \\
    \end{align*}
    Similarly, consider:
    \begin{align*}
        (\textbf{A}^T \textbf{A})^T &= \textbf{A}^T (\textbf{A})^T & \text{transpose of a matrix product} \\
        &= \textbf{A}^T \textbf{A} & \text{double transpose} \\
        &\Rightarrow \textbf{A}^T \textbf{A} \text{ is symmetric by definition} \\
    \end{align*}
\end{enumerate}
\end{document}