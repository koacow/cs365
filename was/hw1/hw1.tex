\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage[norelsize, linesnumbered, ruled, lined, boxed, commentsnumbered]{algorithm2e}
\usepackage{fullpage}

\pdfpagewidth 8.5in
\pdfpageheight 11in 

\topmargin 0in
\oddsidemargin 0in
\evensidemargin 0in

\newtheorem*{claim}{Claim}

\begin{document}

\begin{center}
    \textbf{CS365 Written Assignment 1} \\
    Khoa Cao \\
    Due 
\end{center}

\section*{Question 1}
\begin{proof}
	First, since we pick the largest number from the $k$ numbers we sampled, if at least one of the $k$ numbers is larger than the median, then the largest number we picked must be larger than the median. \\
	Since we are sampling without replacement, the number of ways to sample $k$ numbers from $n$ numbers is:
	\[
		|S| = {n \choose k}
	\]
	The number of numbers, out of the $n$ numbers, that are smaller than or equal to the median is:
	\[
		|E| = {\lfloor n/2 \rfloor}
	\]
	Note that if $k > \lceil n/2 \rceil$, then we are guaranteed to select a number larger than the median, since there are only $\lfloor n/2 \rfloor$ numbers that are smaller than or equal to the median. \\
	Let $E$ be the event that all $k$ numbers we examined are smaller than or equal to the median of the $n$ numbers.
	Then, the probability of us selecting a number larger than the median is:
	\begin{align*}
		Pr(E^c) &= 1 - Pr(E) &\text{complement rule} \\
		&= \begin{cases}
			1 - \frac{|E|}{|S|} & \text{if } k \leq \lfloor n/2 \rfloor \\
			1 & \text{if } k > \lceil n/2 \rceil
		\end{cases} & \text{definition of probability} \\
		&= \begin{cases}
			1 - \frac{{\lceil n/2 \rceil \choose k}}{{n \choose k}} & \text{if } k \leq \lceil n/2 \rceil \\
			1 & \text{if } k > \lceil n/2 \rceil
		\end{cases} & \text{substituting in } |E| \text{ and } |S| \\
	\end{align*}
\end{proof}

\newpage

\section*{Problem 2}
\begin{proof}
	Let $X$ be the RV representing the number of successes we get. Then,
	\begin{align*}
		X &= \sum_{i=1}^{n} X_i & \text{definition of } X \\
		X & \sim Binomial (n, p) & \text{since each coin flip is i.i.d. Bernoulli trial} \\
	\end{align*}
	Examining $\bar{X}$, the average number of successes, we have:
	\begin{align*}
		\bar{X} &= \frac{1}{n} \sum_{i=1}^{n} X_i & \text{definition of } \bar{X} \\
		&= \frac{1}{n} X & \text{definition of } X \\
	\end{align*}
	Using the linearity of expectation, we can compute the expected value of $\bar{X}$:
	\begin{align*}
		E(\bar{X}) &= E(\frac{1}{n} X) & \text{definition of } \bar{X} \\
		&= \frac{1}{n} E(X) & \text{linearity of expectation} \\
		&= \frac{1}{n} (np) & X \sim Binomial(n, p) \\
		&= p & \text{simplifying} \\
	\end{align*}
	Next, we can compute the variance of $\bar{X}$:
	\begin{align*}
		Var(\bar{X}) &= Var(\frac{1}{n} X) & \text{definition of } \bar{X} \\
		&= \left(\frac{1}{n}\right)^2 Var(X) & \text{property of variance} \\
		&= \frac{1}{n^2} (np(1-p)) & X \sim Binomial(n, p) \\
		&= \frac{p(1-p)}{n} & \text{simplifying} \\
	\end{align*}
	Using Chebyshev's inequality, we can give an upper bound on how likely the value of $\bar{X}$ differs from the bias of the coin $p$ by at least $\frac{p}{10}$:
	\begin{align*}
		Pr(|\bar{X} - E(\bar{X})| \geq \frac{p}{10}) &\leq \frac{Var(\bar{X})}{(p/10)^2} & \text{Chebyshev's inequality} \\
		\Rightarrow Pr(|\bar{X} - p| \geq \frac{p}{10}) &\leq \frac{Var(\bar{X})}{(p/10)^2} & \text{substituting in } E(\bar{X}) \\
		&= \frac{\frac{p(1-p)}{n}}{(p/10)^2} & \text{substituting in } Var(\bar{X}) \\
		&= \frac{100(1-p)}{np} & \text{simplifying} \\
	\end{align*}
	Therefore:
	\[
		Pr(|\bar{X} - p| \geq \frac{p}{10}) \leq \frac{100(1-p)}{np}
	\]
\end{proof}

\newpage

\section*{Problem 3}
\begin{claim}
	Let $f$ and $g$ be valid probability distributions defined over the same domain, $S$.
	Then, the convex combination of $f$ and $g$
	\[
		h := \lambda f + (1 - \lambda) g
	\]
	for some $lambda \in [0, 1]$ is also a valid probability distribution.
\end{claim}
\begin{proof}
	To show that $h$ is a valid probability distribution, we need to show that:
	\begin{enumerate}[(i)]
		\item $h(x) \geq 0 \forall x \in S$
		\item $\sum_{x \in S} h(x) = 1$
	\end{enumerate}
	Since the domains of $f$ and $g$ are the same, $h$ is also defined over the same domain, $S$. \\
	For (i), we have:
	\begin{align*}
		h(x) &= \lambda f(x) + (1 - \lambda) g(x) & \text{definition of } h \\
		\sum_{x \in S} h(x) &= \sum_{x \in S} \lambda f(x) + (1 - \lambda) g(x) & \text{applying summation} \\
		&= \sum_{x \in S} \lambda f(x) + \sum_{x \in S} (1 - \lambda) g(x) & \text{linearity of summation} \\
		&= \lambda \sum_{x \in S} f(x) + (1 - \lambda) \sum_{x \in S} g(x) & \text{linearity of summation} \\
		&= \lambda (1) + (1 - \lambda) (1) & \text{since } f \text{ and } g \text{ are valid probability distributions} \\
		&= \lambda + 1 - \lambda & \text{simplifying} \\
		&= 1 & \text{simplifying} \\
	\end{align*}
	Thus, $h$ satisfies (i). \\
	For (ii), we have:
	\begin{align*}
		\lambda f(x) & \geq 0 & \text{since } \lambda \in [0, 1] \text{ and } f(x) \geq 0 \\
		(1 - \lambda) g(x) & \geq 0 & \text{since } (1 - \lambda) \in [0, 1] \text{ and } g(x) \geq 0 \\
		\Rightarrow h(x) &= \lambda f(x) + (1 - \lambda) g(x) \geq 0 + 0 & \text{adding the two inequalities} \\
		&= 0 & \text{simplifying} \\
	\end{align*}
	Thus, $h$ satisfies (ii). \\
	Since $h$ satisfies both (i) and (ii), $h$ is a valid probability distribution.
\end{proof}

\newpage

\section*{Problem 4}


\end{document}